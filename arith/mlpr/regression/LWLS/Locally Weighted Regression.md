# [Locally Weighted Regression](https://www.cnblogs.com/jeromeblog/p/3396486.html)


locally weighted least squares (LWLS) regression


简单回顾一下[线性回归](http://www.cnblogs.com/jeromeblog/p/3396485.html)。我们使用了如下变量：xx—输入变量/特征；yy—目标变量；(x,y)(x,y)—单个训练样本；mm—训练集中的样本数目；nn—特征维度；(x(i),y(i))(x(i),y(i))—第ii个训练样本。在接下来的内容中，仍沿用这些标识。我们给定的模型假设为：




$$
h_θ(x)=θ_0+θ_1x_1+⋯+θ_nx_n=\sum_{i=1}^n{θ_ix_i}=θ=θ^Tx
$$


代价函数定义如下：


$$
J(θ)=\frac{1}{2}\sum_{i=0}^m{(h_θ(x^{(i)})−y^{(i)})^2}
$$



在线性回归问题中，特征的选择会直接影响到模型的性能。如下图所示，仅使用一个特征xx时，很多样本点都没落到直线上，模型的拟合程度不怎么好；如果我们增加一个特征x2x2，拟合效果好很多；如果用5阶多项式来拟合，可以保证图中的6个样本点全部落在曲线上。这种现象不禁让我们想，是不是使用的特征越多越好？答案是否定的。左图存在的问题称之为欠拟合(underfitting)，该模型过于简单，不足以捕捉数据的诸多结构信息；右图存在的问题称之为过拟合(overfitting)，模型设计得过于复杂，虽能完美拟合训练集集上的数据信息，却不能揭示更一般化的规律。

![img](https://images0.cnblogs.com/blog/573996/201310/26123300-a4eba46cc7424ab99c0ac2153acd4eec.png)

下面介绍线性回归的一种改进算法——局部加权回归(Locally Weighted Regression,LWR)。局部加权回归是一种非参数(non-parametric)型学习算法，能在一定程度上将我们从特征选择的困境中拉出来一点。参数型学习算法事先假设数据来自某种模型，然后推断模型参数。这类学习算法通常都有固定数目的参数，一旦学习到参数后，模型也就建立起来了，接下来就能扔掉训练数据并利用该模型独立完成任务。如果事先的假设与实际情况比较接近，模型可以给出比较准确的估计；否则，估计到的参数会有很强的误导性。非参数型学习算法不会事先指定模型，其参数由训练数据决定。这里的非参数不是说完全没有参数，而是指参数数目不是固定的，并且会随训练集规模的增加而增多。此外，我们必须保留整个训练集，以便后续进行完成后续的任务。

![img](https://images0.cnblogs.com/blog/573996/201310/26123350-118570c1917d4743872b7fa4bcff188d.png)

结合上图来阐述局部加权回归的主要思想。绿色的样本点(采样自抛物线)为训练集中的数据。利用线性回归拟合该训练集，最终得到红线表示的线性模型，很显然该模型在训练集上的拟合程度相当差，对特定查询点(Query Point)给出的估计与实际情况相差甚远。局部加权回归则重点考虑查询点附加的样本点(如红色圆形划定的样本集合)，然后在这个样本子集合上执行加权线性回归，得到紫色直线所示的模型。该模型在估计查询点附近的样本时准确度还是很高的。权值是由样本点与查询点间的相似度(在几何空间可用距离度量)决定，越相似的权值越大，差异越大的权值几乎为0(相当于忽略这些样本)。

权值函数有很多种，只要满足两个基本条件即可：1)相似样本间权值较大；2)相似度非常小的样本间权值近似为0。最常用权值函数如下：


$$
w_{(i)}=exp(−\frac{||x(i)−x||_2^2}{2τ^2})
$$


‖x(i)−x‖2为第i个样本x(i)和查询点x之间的欧式距离。显然，当‖x(i)−x‖2很小时，w(i)≈1；当‖x(i)−x‖2很大时，w(i)≈0。参数ττ称为带宽参数(Bandwidth Parameter)，控制权值随距离变量的速度。如下图所示，ττ越小，权值变化越平缓；ττ越大权值变化越快。

![img](https://images0.cnblogs.com/blog/573996/201310/26123441-6c003c0068654164a887891ad7705559.png)

加权线性回归的目标函数形式如下：


$$
J(θ)=\frac{1}{2}\sum_{i=0}^m{w_{(i)}(h_θ(x^{(i)})−y^{(i)})^2}
$$


单个样本(x(j),y(j))(x(j),y(j))情况下的梯度下降问题与线性回归基本相似：


$$
\frac{∂}{∂θj}J(θ)=w^{(j)}(h_θ(x^{(j)})−y^{(j)})⋅x_i
$$


参数迭代更新方式基本不变：


$$
θ_i=θ_i−α\sum_{j=1}^m{w^{(j)}(h_θ(x^{(j)})−y^{(j)})x^{(j)}_i}
$$


一般而言，局部加权回归效果会比线性回归好很多。在训练集很大的情况下，该方法的效率就很低了。因为对应后续的每个查询点，其与训练样本之间的权重都不同。我们首先要计算训练集中所有样本点与查询点直接的权值，然后执行线性拟合过程，并确定适用于该查询点的参数θθ，最后在该查询点处进行估计。

其实，线性回归可视为局部线性回归的一个特例。在线性回归中，所有训练样本的权值都为1，地位平等。因为所有训练样本同等重要，针对不同的查询点就没必要重新计算每个训练样本的权值。虽然模型的性能不如局部线性回归，但是后期我们只需用学习到的参数建立的模型就可以对新样本进行估计，不用每次估计都要耗费大量资源在整个训练集上重新拟合一遍。有得必有失，在这里也体现得淋漓尽致！

以下是实验的截图，[实验代码在这里下载](http://pan.baidu.com/s/1DY740)。左图为局部权值回归，右图为线性回归。绿色曲线由采样自抛物线的样本点组成，蓝色直线为查询点对应的位置，红色直线为学习到的模型。图中的黑色红线是我手动添加的，便于进行对比。从图中可以看出，针对查询点，局部线性回归的误差明显要小于线性回归。

![img](https://images0.cnblogs.com/blog/573996/201310/26123502-362cd7be8f50460785e11fd446291f9e.png)

  **作者：**[JeromeWang](http://www.cnblogs.com/jeromeblog/)
  **邮箱：**[yunfeiwang@hust.edu.cn](mailto:yunfeiwang@hust.edu.cn)
  **出处：**<http://www.cnblogs.com/jeromeblog/>
  **本文版权归作者所有，欢迎转载，未经作者同意必须保留此段声明，且在文章页面明显位置给出原文连接，否则保留追究法律责任的权利。**



分类: [Machine Learning](https://www.cnblogs.com/jeromeblog/category/527632.html)

标签: [Machine Learning](https://www.cnblogs.com/jeromeblog/tag/Machine Learning/), [LWR](https://www.cnblogs.com/jeromeblog/tag/LWR/)