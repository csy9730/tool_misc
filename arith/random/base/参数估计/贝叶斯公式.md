





# 贝叶斯公式



我们期望通过先验概率和似然函数，基于贝叶斯公式计算后验概率。

- P(A)称为先验概率(prior), 事件A发生的概率有多大。该条件一般通过统计获得。
- P(B|A)称为似然(likelihood),在事件A发生的情况下，事件B(或evidence)的概率有多大，该条件一般可以通过因果假设直接得到。
- P(AB) 联合概率，可以通过乘法公式计算
- P(B)称为证据(evidence)，即无论事件如何，事件B(或evidence)的可能性有多大。可以通过全概率公式计算
- P(A|B)称为后验概率(posterior),这是我们需要结合先验概率和证据计算之后才能知道的。



A事件和B事件是关联关系，且A事件是原因，B事件是结果。两者存在近似地因果关系，即A事件易于计算/推导B事件（即已知P(B|A)）；而B事件难以计算/推导A事件（即不知道P(A|B)）。

贝叶斯公式可以根据结果，回溯原因。





贝叶斯公式：
$$
P(A|B)=P(B|A)P(A)/P(B)\\
$$
使用全概率公式对A事件进行拆分
$$
\sum_i{P(A_i)}=1
$$
基于乘法公式，有：
$$
P(B)=P(B)\sum_i{P(A_i)}=\sum_i{P(BA_i)}=\sum_i{P(B|A_i)P(A_i)}
$$
最终形式：
$$
P(A_i|B)=\frac{P(B|A_i)P(A_i)}{\sum_i{P(B|A_i)P(A_i)}}
$$
进而可以推导出MAP最大后验概率估计。
$$
i=argmax_iP(A_i|B)
$$




该公式的意义：已知或假定一组先验条件A，以及A事件发生后B事件的似然函数P(B|A)。通过后验事件B，来推测先验事件A的概率，进而找到最大概率的事件A_i。



|        | P(A_1)    | P(A_2)    |
| ------ | --------- | --------- |
| P(B_1) | P(A_1B_1) | P(A_2B_1) |
| P(B_2) | P(A_1B_2) | P(A_2B_2) |
|        |           |           |



- 边缘概率
- 联合概率
- 条件概率

## demo

考虑以下一个简单问题

已知有两个布袋，一个布袋有7个红球，3个黑球；第二个布袋有4个红球，6个黑球。
已知从两个布袋中挑一个布袋，从一个布袋中摸出了一个红球，请问该布袋是第一个，第二个的概率？
$$
P(A_1)=0.5,P(A_2)=0.5\\
P(B=red|A_1)=0.7\\
P(B=red|A_2)=0.4\\
$$
使用贝叶斯公式：
$$
P(A_1|B=red)=\frac{P(B=red|A_1)P(A_1)}{P(B=red|A_1)P(A_1)+P(B=red|A_2)P(A_2)}=0.7/(0.7+0.4)=7/11\\
P(A_2|B=red)=\frac{P(B=red|A_1)P(A_1)}{P(B=red|A_1)P(A_1)+P(B=red|A_2)P(A_2)}=0.4/(0.7+0.4)=4/11\\
$$


以上问题中，A事件是离散概型，B事件也是离散概型，而且是单次观测事件。

### 2

可以考虑扩展问题

- A扩展成连续概型，B是离散概型
- B扩展成连续概型。
- 如果把B事件改成多次观测，问题就变成了隐马科夫模型HMM。


$$
\frac{P(B=red|A_1)P(A_1)}{P(B=red|A_1)P(A_1)+P(B=red|A_2)P(A_2)}
$$
