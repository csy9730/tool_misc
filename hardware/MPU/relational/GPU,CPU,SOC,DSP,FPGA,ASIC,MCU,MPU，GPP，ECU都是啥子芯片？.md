# GPU,CPU,SOC,DSP,FPGA,ASIC,MCU,MPU，GPP，ECU都是啥子芯片？

[殷玮](https://www.zhihu.com/people/becausehome)

汽车话题下的优秀答主



78 人赞同了该文章

上次关于TOPS的问题，似乎得到很多的关注，这里就多讲点自动驾驶芯片的内容

https://zhuanlan.zhihu.com/p/343191353zhuanlan.zhihu.com![图标](https://zhstatic.zhihu.com/assets/zhihu/editor/zhihu-card-default.svg)

上次也聊到了，讨论自动驾驶芯片的意义并非单纯理解芯片原理，更需要理解计算并非是一个软件工作而是一个软硬件配合的工作。很多算法在不同的芯片上都可以实施，但是量产过程中需要在灵活性以及成本功耗之间做出权衡。而如果要做到这点，你必须理解芯片。

### 芯片的概念结构

当下芯片结构是复杂的，但简单划分就三种概念结构。冯.诺依曼结构，哈弗结构和改进哈弗结构。**了解一个芯片的结构核心是看它的总线布置和存储器设计**。

![img](https://pic1.zhimg.com/80/v2-4e579501944da58fce6006315923ce90_720w.jpg)

打个比方，假如芯片是一个“银行”，这个银行很小就两个人。一个负责拨算盘（计算），其他它都不管。一个负责记录文档（存储），把“算什么？”（数据）和“咋么算？”（程序）都完整的用文档记录下来，方便和拨算盘的人沟通。他们之间定义了一个沟通方式（一组总线），内容包括了“文档从哪里取？放回哪里？”（寻址总线）和“文档具体内容是什么？”（数据总线）两大部分，换句话说，一组总线包括一个地址总线和一个数据总线。特别注意这里的总线上的“数据”对应着一份文档，不要和文档里 “算什么？”的这个数据概念混淆了。

客户把原始的需求文档（包含输入数据和程序）交给负责记录的人后，根据定义的沟通方式，和负责实际计算的人一起反复来回的传递文档（拿程序指令，拿数据，返结果），最后把最终的计算结果通过记录的人反馈给客户。上面这种分工思路就是冯.诺依曼结构的核心，**关键的特点就是客户只用和一个记录人员沟通就行，再复杂的需求都可以一股脑的给一个人，剩下的都是他们之间的事。整个过程非常灵活，这也是冯.诺依曼结构的最大优势。**这种结构下程序指令存储地址和数据存储地址指向同一存储器的不同物理位置，因此程序指令和数据的宽度相同。**但这种方式效率不高，因为记录的人每个时刻只能干一件事，要么告诉拨算盘的人咋么算，要么告诉它算什么。因此出现了哈佛结构**，将程序指令和数据分开存储，指令和数据可以有不同的数据宽度。采用了独立的一组程序总线和一组数据总线。这就相当于原来2个人，现在三个人，记录员分了个工，一个就负责记录传递计算方法（程序），一个人就负责记录传递计算内容（数据）。两个人和拨算盘的人点对点沟通，但相互之间老死不往来。这种并行化自然提高了效率，原来要至少两个周期做完的事，一个周期就搞定了。可这种方法让客户要同时对应2个毫无联系的记录员这非常不友好。且如果需求侧重点不同，两个人的工作量常常不均衡导致浪费，如果由一个人担当（冯.诺依曼结构），这些问题都不存在。

**为了解决这个平衡问题出现了改进哈弗结构。它只有一组总线供程序存储器和数据存储器分时共用。**原来的哈佛结构需要４条（2组）总线，改进后需要两条（1组）总线，且保留了两个独立并行的存储器。也就是说，记录员还是为了效率做了分工，但沟通方式升级，走上了“敏捷之路”**。不再是两两沟通的老国企做派，把客户和拨算盘的人也加入进来做了个四方沟通会（分时公用），归口统一保证了灵活性，同时分时也对并行化影响不大。**

讨论完基本的结构思想这里有几点要着重提醒下。实际的芯片设计是对这几种概念结构的扩展和嵌套，比如CPU处理器虽然外部总线上看是诺依曼结构，但是由于内部高速缓存的二级设计，实际上对内已经算是改进哈佛结构了。

## 芯片的派系划分

接着我们聊下芯片的分类，梳理分类首先要区分芯片的两个大类通用芯片（CPU, GPU, DSP等）和定制芯片（FPGA, ASIC等），这个大类划分很重要，两者有本质上的不同。同样用银行做比喻，通用芯片就是“银行柜员”而定制芯片就是“ATM机”。

通用芯片关键是“通用”二字，这意味着其必须具备处理各式各样千奇百怪的指令要求，并且经常同时存在多个外部设备的请求，它必须拥有随时中止目前的运算转而进行其他运算，完成后再从中断点继续当前运算的能力。就好比银行柜员，客户要办的业务千奇百怪，时常还来个缺德的插队骂娘或者站着位置不走撩你两下的人存在。柜员都要应对。而为了做到这一点通用芯片有复杂的控制取指译码流程，Cache内存分级机制（缓和高速CPU与低速内存的临时指令存储器），真正的计算单元ALU只占了通用芯片不大的一部分，更多设计是为了灵活性存在的，在计算效率和通用性上的权衡上牺牲前者选择后者。

> 计算机元件无法理解我们的指令，它们只能理解晶体管实现的两种状态：“开”和“关”的含义，对应的就是1和0这,为了让指令变成CPU能理解的0和1，CPU需要一个专门的译码器来翻译我们的指令。这个过程分为两步：“取指”（从一个专门存放指令的存储器中将需要执行的指令提取出来）和“译码”（根据特定的规则将指令翻译成计算单元能够理解的数据）。

当我们在上文讨论芯片结构的时候更多的是在讨论通用芯片的结构，是在讨论说满足客户变化需求的时候，哪种“人员组织形式和沟通方式”是最高效的。

而定制芯片就是完全的另一个概念，虽然它也有结构思想在里面，但是就像你不会去讨论一个程序的“人员组织架构”一样，在这种芯片里根本就没有时序中断，取指译码这些为了灵活性而设计的概念。相比通用芯片，定制芯片是没有“人性”的，就是一个ATM机，其给客户定义了清晰的操作流程，省去了中断等大量灵活性设计，撩小姐姐的一套对机器人是不成立。

打个比方，比如一个比大小的逻辑用冯诺依曼结构的CPU至少需要几条指令完成，但用FPGA就根本不用考虑时序周期，只要串联几个逻辑单元，在一个周期就搞定了。但如果再增加几个逻辑，CPU还是在相同逻辑资源下用几个指令完成，但是FPGA 就需要额外占用另一部分逻辑资源完成计算。再比如FPGA和GPU（GPU是通用芯片）在并行化上有类似的思想，但两者实际没有多少可比性，你不会把三个柜员的办理通道和ATM机理存在在三个恰好并行的流程做比较一样。还有人在信号处理效率上把DSP和FPGA拿出来对比，我觉得这些零零种种的比较都没有太大意义。为灵活性存在的“人”（通用芯片），和为效率存在的“机器”（定制芯片）是两个维度的事情，不要从性能上去强行比较。

从这里我们可以大致看出来，两者的几个重要差异。定制芯片是对已经固化的业务进行降本增效，就像银行用ATM机，代替成本更高的柜员处理一些常规银行业务。而通用芯片是为了对一些无法或者暂时没有固化的业务作出的灵活设计。两者没有优劣之分。

了解了芯片的两个大方向，我们看下这两个大方向内部的细分差异以及联系。通用芯片下的CPU（MPU）, GPU, DSP，MCU之间同样存在细分差异。

![img](https://pic2.zhimg.com/80/v2-8d5e1e4b3b0adddb00f982eabc663965_720w.jpg)

CPU和MPU可以简单理解为一个概念，只是理解范畴上的区别。CPU和GPU之间的区别更多的是核的数量。CPU虽然有多核，但基本不超过两位数，每个核都有足够大的缓存和足够多的数字和逻辑运算单元，并有更复杂的逻辑判断硬件，就像银行里常备的3-4个柜台的柜员，擅长处理客户很复杂的业务。而GPU的核数远超CPU每个核拥有的缓存大小相对小，数字逻辑运算单元也少而简单，更像是500多个电话客服柜员，处理一些相对简单但数量众多的客户业务。

DSP（数字信号处理芯片）是一类特殊的CPU，采用了上面说的哈佛结构，且存在专用的硬件算法电路和专门的寻址模式。它具有通用芯片设计的灵活性，但在实时运算过程中很少变化，因此特化了业务流程的性能（记录和计算过程）。就像是某个办理“外汇存取”的专业柜台会部署一些特化的柜员和流程。DSP对于专用信号（视频编解码，通讯信号）的处理能力远远的优于一般CPU。当然普通柜台也可以处理展业柜台的业务，但性价比就很差了，如果需求很多开设专门的柜台就变得有意义，这些还是和客户需求有关。用DSP处理专门的信号流常具有执行时间可控，芯片性价比高等优点。

讲完了通用芯片，定制芯片也有两个主要方向，FPGA和ASIC。两者核心的区别就是固化程度。FPGA仍然具有一定的灵活性（但远逊于通用芯片），而ASIC则是完成固化的设计（也存在和FPGA类似的部分编辑的产品存在）。类似可以编程的ATM机和完全固化的ATM机，两者区别最大的维度还是成本和功耗。

![img](https://pic2.zhimg.com/80/v2-b612b53d884a376c69de990fdc3afb99_720w.jpg)

FPGA最早是从专用集成电路发展而来的半定制化的可编程电路，是高端的CPLD (Complex Programmable Logic Device复杂可编程逻辑器件)。FPGA可以实现一个DSP, GPU甚至是CPU的功能，就像之前说的把柜员业务固化为ATM机操作流程一样。但不是说FPGA可以代替CPU，这是设计目的上的大方向差异，反复强调。

FPGA是一堆逻辑门，通过硬件描述语言HDL把它转成电路连接，从最基本的逻辑门层面上连接成电路。虽然看起来像一块CPU，其实是完全硬件实现的。 根据一个固定的模式来处理输入的数据然后输出。FPGA片上大部分都是计算单元，没有控制单元并不代表FPGA不会执行指令，事实上FPGA里控制单元的角色由单元和单元之间可编程逻辑连接线来完成的，通过HDL编程更改每个单元的运算逻辑和单元之间的连接方式，从而使其达到和一般的运行程序差不多的效果。由于省去了CPU的取指和译码两个步骤，FPGA重复运行相同代码的效率得到了极大的提高，也因此，其无法应对没有被编程过的指令。

ASIC就是专用IC，没有明确的定义。可以理解为除了单片机、DSP、FPGA之类的能叫出名的IC，剩下的都是ASIC。ASIC原本就是专门为某一项功能开发的专用集成芯片。后来ASIC发展了一些，称为半定制专用集成电路，相对来说更接近FPGA，甚至在某些地方，ASIC是个大概念，FPGA属于ASIC的一部分，也常常被作为ASIC开发的预研。其代表了在需求一定的情况下，对性价比的极致追求。

## 芯片之上的集成

**在上面我偷偷遗漏了一个概念MCU，原因是其本身不是一种芯片类型而是一种集成方式，SOC芯片也是同样的道理，两者的区别是程度上的不同。**在自动驾驶汽车领域MCU更多的是集成了更多的输入和输出设备在芯片当中，方便更好的控制，因此叫做微控制器而不是微处理器。而SOC是在更高的层面上将不同的芯片做了进一步的集成，维度更高。如果MCU是一种人员组织最终形成一个公司对外服务，那SOC更像是公司级别的组织形成了一个行业对外服务。

单片机是MCU的通俗说法，经典的51系列就是一堆IO口，后来慢慢的把常用的PWM, AD之类的功能加入了单片机之中。其构成等价于一个带了更多外设CPU，但侧重点是讨论其外设的部分。**在PWM,AD等之上继续发展其外设也就形成了汽车行业熟悉的ECU即电子控制单元，同时泛指汽车上所有电子控制系统，可以是转向ECU，空调ECU等。**

ECU一般由MCU，扩展内存，扩展输入和输出（CAN/LIN，AD,PWM等），电源电路和其他一些电子元器件组成，特定功能的ECU还带有诸如红外线收发器、脉冲发生器，强弱电隔离等元器件。整块电路板设计安装与一个铝质盒内，通过卡扣或者螺钉方便安装于车身钣金上。

![img](https://pic1.zhimg.com/80/v2-91c52945793e4724199dc343d74391c4_720w.jpg)

在输入处理电路中，ECU的输入信号主要有三种形式，模拟信号、数字信号(包括开关信号)、脉冲信号。模拟信号通过A/D转换为数字信号提供给微处理器。

在输出电路中，微处理器输出的信号往往用作控制电磁阀、指示灯、步进电机等执行件。微处理器输出信号功率小，使用+5v的电压，汽车上执行机构的电源大多数是蓄电池，需要将微处理器的控制信号通过输出处理电路处理后(D/A,放大等)再驱动执行机构。

电源电路中，传统车的ECU一般带有电池和内置电源电路，以保证微处理器及其接口电路工作在+5v的电压下。即使蓄电池电压有较大波动时，也能提供稳定电压保证系统的正常工作。

一般搭载8位MCU的ECU主要应用于风扇控制、空调控制、雨刷、天窗、门控等较低阶的控制功能。16位MCU主要应用如引擎控制、齿轮与离合器控制等。32位MCU应用于多媒体信息系统，实时性的安全动力系统以及复杂的X-by-wire等传动功能。更复杂的功能就不在MCU或者ECU的讨论范围内了。

随着自动驾驶的发展，ECU的概念进一步升级，更为流行的说法是域控制器，其无外乎就是把MCU变成了SoC（片上系统），同时集成了更多的外围设备而已。目前域控制器搭载的主流通用芯片（GPP）多采用SoC的芯片设计方法，通过HDL语言在SoC内由电路集成各种功能芯片。在SoC中各种组件（IP核）采用类似搭积木的方法组合在一起。IP核（诸如典型的ARM内核设计技术）被授权给数百家半导体厂商，做成不同的SoC芯片。还可能集成GPU、编解码器（DSP）、GPS、WiFi蓝牙基带等一系列功能。如果看一下高通或者TI的芯片，基本是一个ARM核控制整体运算，一个DSP处理语音编解码， 一个GPU负责图像运算，一个基带和天线处理模块负责通信，以及GPS，安全加密等林林总总的特殊芯片。

过去极端情况下自动驾驶的原型处理器功耗可以高达5000W，不仅昂贵且需要搭载额外的散热装置。SoC和ASIC的发展给我们带来很多启示，回到我经常提及的贯穿整个自动驾驶系统的灵活性。在新的SoC世界里，你不会从不同的供应商那里组装物理元件。相反，你从不同的供应商那里组装IP从而获得更好的集成度，也因此更容易降低功耗和成本。

## 软硬件的匹配设计

大部分自动驾驶算法公司都想定制或自制ASIC/SOC计算平台，原因还有另一个层面来源于软硬件的匹配问题。算法的性能与硬件设计往往脱离不开。追求模块化就要牺牲利用率。要提高利用率就需要软硬件一体设计。你的算法是用GPU合适还是CPU合适，网络模型一次用多少内存又同时使用多少MAC，由此来设计芯片。或者说反过来给定一个芯片，我的算法要如何兼容，是否要减少内存访问次数提高利用率，还是要迁移部分CPU基于规则的算法，改为用GPU基于深度学习来实现。软硬件一起考虑往往才能充分利用好系统性能。

不同的芯片，不同的算法和需求，往往有最优的组合方式。比如一个经典底层而常用的算法应用，需求是大量的且竞争是激烈的时候，ASIC就是很好的选择。为了一个简单功能（比如编解码）支付一个ARM的授权是愚蠢的。

如果算法非常经典且底层，但仍然有改进的空间和需要适配的不同场景，信号流的处理（手机语音处理）可以直接使用DSP，而更复杂的输入输出逻辑算法（比如图像SIFT特征处理），就可以交给FPGA来做，性能相对于CPU都可以由30-100倍的提升，且成本和耗能更小。

在复杂算法领域相对于CPU，GPU的众核架构把同样的指令流并行发送到众核上，采用不同的输入数据执行。所以GPU比CPU更适合并行算法，而串行的复杂规则逻辑则更适合CPU处理。更具体的说，如果标量视为零阶张量，矢量视为一阶张量，矩阵视为二阶张量。CPU对应标量计算，主要是路径规划和决策类算法，常用的传感器融合如卡尔曼滤波算法也多是标量运算。GPU则对应矢量或者说向量计算，包括点云，地图，深度学习，核心是矩阵运算。用CPU编写程序时，更适合通过精益化逻辑来提升性能。而用GPU编写程序时，则更合适利用算法并发处理来提升性能。

**以上，对芯片的理解到这个程度一般工作就能够开展了，如果对自动驾驶感兴趣，欢迎关注我的专栏。**

自动驾驶量产经验汇总www.zhihu.com![图标](https://pic3.zhimg.com/v2-382694943d1a68529c8863b4bd59bf72_ipico.jpg)

![img](https://pic3.zhimg.com/v2-9742eb5063372e42cbccdca12580bf02_b.jpg)



编辑于 01-17

自动驾驶

芯片（集成电路）